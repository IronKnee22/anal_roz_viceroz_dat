## Co je to kvartil?âœ”ï¸
**Kvartil je hodnota, kterÃ¡ dÄ›lÃ­ seÅ™azenÃ¡ data na ÄtyÅ™i stejnÃ© ÄÃ¡sti. ExistujÃ­ tÅ™i kvartily:**
1. kvartil (Q1): hodnota, pod kterou leÅ¾Ã­ 25 % dat
2. kvartil (Q2): mediÃ¡n, dÄ›lÃ­ data na poloviny
3. kvartil (Q3): hodnota, pod kterou leÅ¾Ã­ 75 % dat  

Kvartily se pouÅ¾Ã­vajÃ­ k popisu rozloÅ¾enÃ­ dat a k detekci odlehlÃ½ch hodnot.

## JakÃ½ je rozdÃ­l mezi vÃ½bÄ›rovÃ½m prÅ¯mÄ›rem a mediÃ¡nem?âœ”ï¸
**VÃ½bÄ›rovÃ½ prÅ¯mÄ›r:**  
SouÄet vÅ¡ech hodnot ve vÃ½bÄ›ru dÄ›lenÃ½ jejich poÄtem. OvlivnÄ›n extrÃ©mnÃ­mi hodnotami (odlehlÃ½mi daty).

**MediÃ¡n:**  
StÅ™ednÃ­ hodnota seÅ™azenÃ©ho vÃ½bÄ›ru. NenÃ­ ovlivnÄ›n odlehlÃ½mi hodnotami.

**RozdÃ­l:**  
PrÅ¯mÄ›r odrÃ¡Å¾Ã­ celkovou hodnotu dat, ale je citlivÃ½ na extrÃ©my, zatÃ­mco mediÃ¡n lÃ©pe reprezentuje â€typickouâ€œ hodnotu v pÅ™Ã­padÄ› asymetrickÃ©ho rozdÄ›lenÃ­.

## Co je to histogram?âœ”ï¸
**Histogram** je grafickÃ© znÃ¡zornÄ›nÃ­ rozdÄ›lenÃ­ numerickÃ½ch dat. Osa X zobrazuje intervaly hodnot (tzv. tÅ™Ã­dy), osa Y poÄet (frekvenci) hodnot v kaÅ¾dÃ© tÅ™Ã­dÄ›. Sloupce jsou spojitÃ© â€“ bez mezer. Histogram slouÅ¾Ã­ k posouzenÃ­ tvaru rozdÄ›lenÃ­ dat (napÅ™. symetrie, Å¡piÄatost, pÅ™Ã­tomnost odlehlÃ½ch hodnot).

## Jak se zobrazuje odlehlÃ¡ hodnota v box grafu?âœ”ï¸
OdlehlÃ¡ hodnota v box grafu (boxplotu) se zobrazuje jako samostatnÃ½ bod (teÄka nebo kÅ™Ã­Å¾ek) mimo â€vousyâ€œ grafu.
Je definovÃ¡na jako hodnota leÅ¾Ã­cÃ­ mimo interval:  
- Q1âˆ’1.5Ã—IQR,Q3+1.5Ã—IQR,
- kde IQR = Q3 - Q1 (interkvartilovÃ© rozpÄ›tÃ­).

## HierarchickÃ© shlukovÃ¡nÃ­ âœ”ï¸
**HierarchickÃ© shlukovÃ¡nÃ­** (hierarchical clustering) je metoda shlukovÃ© analÃ½zy, kterÃ¡ seskupuje data do stromovÃ© struktury (dendrogramu), podle jejich podobnosti. CÃ­lem je uspoÅ™Ã¡dat objekty do hierarchie â€“ tedy do â€stromuâ€œ, kde blÃ­Å¾e pÅ™Ã­buznÃ© objekty tvoÅ™Ã­ menÅ¡Ã­ skupiny a ty se postupnÄ› spojujÃ­ do vÄ›tÅ¡Ã­ch celkÅ¯.

## ShlukovÃ¡nÃ­ rozklademâœ”ï¸
**ShlukovÃ¡nÃ­ rozkladem** (partitioning clustering) je metoda, kterÃ¡ rozdÄ›lÃ­ mnoÅ¾inu dat do pevnÄ› danÃ©ho poÄtu K shlukÅ¯ (K je zadÃ¡no pÅ™edem).

NejznÃ¡mÄ›jÅ¡Ã­ metoda:  
K-means (nejbliÅ¾Å¡Ã­ stÅ™ed):
- ZvolÃ­ se K poÄÃ¡teÄnÃ­ch stÅ™edÅ¯ (nÃ¡hodnÄ›).
- KaÅ¾dÃ½ bod se pÅ™iÅ™adÃ­ ke stÅ™edu, ke kterÃ©mu mÃ¡ nejmenÅ¡Ã­ vzdÃ¡lenost.
- SpoÄÃ­tajÃ­ se novÃ© stÅ™edy jako prÅ¯mÄ›r bodÅ¯ ve shluku.
- Opakuje se, dokud se stÅ™edy nezmÄ›nÃ­ (konvergence).

## Co je shlukovÃ¡ analÃ½za?âœ”ï¸
**ShlukovÃ¡ analÃ½za** je metoda neÅ™Ã­zenÃ©ho uÄenÃ­, kterÃ¡ slouÅ¾Ã­ k rozdÄ›lenÃ­ dat do skupin (shlukÅ¯) tak, aby si data uvnitÅ™ shluku byla co nejpodobnÄ›jÅ¡Ã­ a mezi shluky co nejodliÅ¡nÄ›jÅ¡Ã­.

## VysvÄ›tlete proÄ je nutnÃ¡ normalizace dat pÅ™i shlukovÃ¡nÃ­.âœ”ï¸
Normalizace dat pÅ™i shlukovÃ¡nÃ­ je nutnÃ¡, protoÅ¾e:
- RÅ¯znÃ© promÄ›nnÃ© mohou mÃ­t rÅ¯znÃ½ rozsah hodnot (napÅ™. vÄ›k 0â€“100, pÅ™Ã­jem 0â€“100â€¯000).
- Bez normalizace by promÄ›nnÃ© s vÄ›tÅ¡Ã­m rozsahem dominovaly pÅ™i vÃ½poÄtu vzdÃ¡lenostÃ­, kterÃ© jsou zÃ¡kladem vÄ›tÅ¡iny shlukovacÃ­ch metod.
- Normalizace (napÅ™. z-skÃ³re nebo min-max) zajistÃ­, Å¾e vÅ¡echny promÄ›nnÃ© majÃ­ stejnou vÃ¡hu pÅ™i vÃ½poÄtu vzdÃ¡lenostÃ­.

## ZÃ¡kladnÃ­ algoritmus ID3âŒ
ID3 vytvÃ¡Å™Ã­ rozhodovacÃ­ stromy a pouÅ¾Ã­vÃ¡ entropii a informaÄnÃ­ zisk k vÃ½bÄ›ru nejvhodnÄ›jÅ¡Ã­ch atributÅ¯.

## EntropieâŒ
Je to mÃ­ra nejistoty nebo prÅ¯mÄ›rnÃ© informace, kterou dostaneme pÅ™i pozorovÃ¡nÃ­ nÃ¡hodnÃ© promÄ›nnÃ©

## 1. Co je to perceptronâœ”ï¸

Perceptron je **jednoduchÃ½ neuronovÃ½ model** pro binÃ¡rnÃ­ klasifikaci. Vstupy nÃ¡sobÃ­ vÃ¡hami, sÄÃ­tajÃ­ se a vÃ½stup se urÄÃ­ pomocÃ­ aktivaÄnÃ­ funkce (napÅ™. signum).

---

#### **a. Jak se v nÄ›m vybavuje odpovÄ›Ä? Co je jeho odpovÄ›dÃ­?**

* Vstupy $x_i$ se vynÃ¡sobÃ­ vÃ¡hami $w_i$, spoÄÃ­tÃ¡ se souÄet $\sum w_i x_i + b$ (bias).
* Tento souÄet se vloÅ¾Ã­ do **aktivaÄnÃ­ funkce**.
* OdpovÄ›Ä je typicky **0 nebo 1** (binÃ¡rnÃ­ vÃ½stup).

---

#### **b. Jak se perceptron uÄÃ­?**

* ProchÃ¡zÃ­ trÃ©novacÃ­ data a upravuje vÃ¡hy podle **chyby mezi vÃ½stupem a cÃ­lovou hodnotou**.
* UÄenÃ­ probÃ­hÃ¡ pomocÃ­ **delta pravidla**:

  $$
  w_i \leftarrow w_i + \eta \cdot (t - y) \cdot x_i
  $$

  kde $\eta$ je **uÄÃ­cÃ­ rychlost**, $t$ cÃ­l a $y$ vÃ½stup.

---

#### **c. K Äemu se dÃ¡ pouÅ¾Ã­t?**

* K **binÃ¡rnÃ­ klasifikaci** lineÃ¡rnÄ› separovatelnÃ½ch dat.
* NapÅ™. rozpoznÃ¡vÃ¡nÃ­ jednoduchÃ½ch vzorÅ¯.

---

## 2. Co je to vÃ­cevrstvÃ¡ perceptronovÃ¡ sÃ­Å¥ (MLP)?âœ”ï¸

SÃ­Å¥ sloÅ¾enÃ¡ z **vÃ­ce vrstev perceptronÅ¯** â€“ vstupnÃ­, skrytÃ½ch a vÃ½stupnÃ­ vrstvy.
UmÃ­ **modelovat i nelineÃ¡rnÃ­ vztahy** dÃ­ky nelineÃ¡rnÃ­m aktivaÄnÃ­m funkcÃ­m.

---

#### **a. Jak se volÃ­ poÄty neuronÅ¯ v jednotlivÃ½ch vrstvÃ¡ch?**

* **VstupnÃ­ vrstva:** podle poÄtu atributÅ¯.
* **VÃ½stupnÃ­ vrstva:** podle poÄtu tÅ™Ã­d (napÅ™. 1 neuron pro binÃ¡rnÃ­ klasifikaci).
* **SkrytÃ© vrstvy:** volÃ­ se experimentÃ¡lnÄ› nebo podle heuristik (napÅ™. 1â€“2 skrytÃ© vrstvy, poÄet neuronÅ¯ mezi vstupem a vÃ½stupem).

---

#### **b. Jak probÃ­hÃ¡ uÄenÃ­ pomocÃ­ algoritmu Backpropagation?**

1. **DopÅ™ednÃ½ prÅ¯chod:** spoÄÃ­tÃ¡ se vÃ½stup sÃ­tÄ›.
2. **SpoÄtenÃ­ chyby:** porovnÃ¡ se s cÃ­lovou hodnotou.
3. **ZpÄ›tnÃ½ prÅ¯chod:** chyba se Å¡Ã­Å™Ã­ zpÄ›t do sÃ­tÄ› a podle nÃ­ se **aktualizujÃ­ vÃ¡hy** pomocÃ­ gradientnÃ­ho sestupu.
4. Proces se **opakovanÄ› iteruje** pÅ™es trÃ©novacÃ­ data, dokud se chyba nezmenÅ¡Ã­.

---

**ShrnutÃ­:**

* Perceptron = zÃ¡kladnÃ­ neuron pro binÃ¡rnÃ­ klasifikaci.
* MLP = vÃ­cevrstvÃ¡ sÃ­Å¥ pro sloÅ¾itÄ›jÅ¡Ã­ Ãºlohy, uÄÃ­ se pomocÃ­ backpropagation.


## PCAâŒ
AnalÃ½za hlavnÃ­ch komponent (PCA â€“ Principal Component Analysis) je statistickÃ¡ metoda pouÅ¾Ã­vanÃ¡ ke snÃ­Å¾enÃ­ rozmÄ›rnosti dat, aniÅ¾ by doÅ¡lo k vÃ½raznÃ© ztrÃ¡tÄ› informace. JejÃ­m hlavnÃ­m cÃ­lem je najÃ­t novÃ© promÄ›nnÃ© â€“ tzv. hlavnÃ­ komponenty â€“ kterÃ© co nejlÃ©pe vystihujÃ­ variabilitu pÅ¯vodnÃ­ch dat. Tyto komponenty jsou lineÃ¡rnÃ­ kombinace pÅ¯vodnÃ­ch promÄ›nnÃ½ch, jsou na sobÄ› nezÃ¡vislÃ© (ortogonÃ¡lnÃ­) a jsou seÅ™azeny podle toho, kolik variability v datech vysvÄ›tlujÃ­ â€“ prvnÃ­ komponenta zachycuje nejvÄ›tÅ¡Ã­ moÅ¾nou ÄÃ¡st rozptylu, druhÃ¡ o nÄ›co menÅ¡Ã­, a tak dÃ¡le.

Zde jsou struÄnÃ© a jasnÃ© odpovÄ›di vhodnÃ© pro test:

---

## 1. Co je to podpora (support) mnoÅ¾iny poloÅ¾ek?

**Podpora** je poÄet (nebo podÃ­l) transakcÃ­, kterÃ© obsahujÃ­ danou mnoÅ¾inu poloÅ¾ek.

$$
\text{Support}(X) = \frac{\text{poÄet transakcÃ­ obsahujÃ­cÃ­ch } X}{\text{celkovÃ½ poÄet transakcÃ­}}
$$

---

## 2. Co je mnoÅ¾ina ÄastÃ½ch poloÅ¾ek a jak je definovÃ¡na?

**ÄŒastÃ¡ mnoÅ¾ina poloÅ¾ek** je takovÃ¡, jejÃ­Å¾ podpora je **vÄ›tÅ¡Ã­ nebo rovna zadanÃ© minimÃ¡lnÃ­ podpoÅ™e**.

$$
\text{Support}(X) \geq \text{minsup}
$$

---

## 3. Jak byste definovali â€œapriori vlastnost podmnoÅ¾inâ€?

**Apriori vlastnost:**
Pokud je mnoÅ¾ina poloÅ¾ek ÄastÃ¡, **vÅ¡echny jejÃ­ podmnoÅ¾iny musÃ­ bÃ½t takÃ© ÄastÃ©**.

---

## 4. Jak funguje apriori algoritmus?

1. **Najde vÅ¡echny ÄastÃ© 1-poloÅ¾ky** (podle minsupu).
2. **Generuje kandidÃ¡ty pro 2-poloÅ¾ky** kombinacÃ­ ÄastÃ½ch 1-poloÅ¾ek.
3. **OdstranÃ­ kandidÃ¡ty**, jejichÅ¾ podmnoÅ¾iny nejsou ÄastÃ© (apriori vlastnost).
4. **SpoÄÃ­tÃ¡ podporu zbÃ½vajÃ­cÃ­ch kandidÃ¡tÅ¯**.
5. **Opakuje** pro 3-poloÅ¾ky, 4-poloÅ¾ky atd., dokud nevzniknou Å¾Ã¡dnÃ­ novÃ­ kandidÃ¡ti.

---

ğŸ“Œ **CÃ­lem je najÃ­t vÅ¡echny ÄastÃ© mnoÅ¾iny poloÅ¾ek** bez zbyteÄnÃ©ho prohledÃ¡vÃ¡nÃ­ nepravdÄ›podobnÃ½ch kombinacÃ­.

