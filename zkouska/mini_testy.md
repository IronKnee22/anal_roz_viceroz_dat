âœ”ï¸âŒ

## PopiÅ¡te rozdÃ­l mezi informovanÃ½mi (uÄenÃ­ s uÄitelem) a neinformovanÃ½mi (uÄenÃ­ bez uÄitele) metodami data miningu?âœ”ï¸
**InformovanÃ© metody (uÄenÃ­ s uÄitelem):**  
Metody, kterÃ© pracujÃ­ s daty, kde kaÅ¾dÃ½ zÃ¡znam mÃ¡ pÅ™iÅ™azenou cÃ­lovou promÄ›nnou (label). CÃ­lem je nauÄit model predikovat tuto promÄ›nnou. PÅ™Ã­klady: klasifikace, regrese.

**NeinformovanÃ© metody (uÄenÃ­ bez uÄitele):**
Metody, kterÃ© pracujÃ­ s daty bez cÃ­lovÃ© promÄ›nnÃ©. CÃ­lem je nalÃ©zt strukturu nebo vzory v datech. PÅ™Ã­klady: shlukovÃ¡nÃ­, detekce odlehlÃ½ch hodnot, asociaÄnÃ­ pravidla.

## PopiÅ¡te rozdÃ­l mezi numerickou, kategorickou a ordinÃ¡lnÃ­ promÄ›nnou, pÅ™Ã­padnÄ› uveÄte pÅ™Ã­klad.âœ”ï¸
**NumerickÃ¡ promÄ›nnÃ¡:**  
MÄ›Å™itelnÃ¡ veliÄina, mÃ¡ ÄÃ­selnÃ© hodnoty, mezi nimiÅ¾ lze poÄÃ­tat rozdÃ­ly.  
PÅ™Ã­klad: vÄ›k, vÃ½Å¡ka, teplota.

**KategorickÃ¡ promÄ›nnÃ¡:**  
Popisuje kategorie bez pÅ™irozenÃ©ho poÅ™adÃ­.  
PÅ™Ã­klad: barva oÄÃ­ (modrÃ¡, hnÄ›dÃ¡, zelenÃ¡).

**OrdinÃ¡lnÃ­ promÄ›nnÃ¡:**  
KategorickÃ¡ promÄ›nnÃ¡ s pÅ™irozenÃ½m poÅ™adÃ­m, ale bez znÃ¡mÃ½ch odstupÅ¯ mezi kategoriemi.  
PÅ™Ã­klad: znÃ¡mka ve Å¡kole (vÃ½bornÃ½, chvalitebnÃ½, dobrÃ½, dostateÄnÃ½, nedostateÄnÃ½).

## ShlukovÃ¡nÃ­ (clustering). StruÄnÄ› popiÅ¡te dva zÃ¡kladnÃ­ pÅ™Ã­stupy ke tvorbÄ› shlukÅ¯âœ”ï¸
1. **HierarchickÃ© shlukovÃ¡nÃ­:**
VytvÃ¡Å™Ã­ hierarchii shlukÅ¯ (stromovou strukturu). MÅ¯Å¾e bÃ½t:

- AgregativnÃ­: zaÄÃ­nÃ¡ s jednotlivÃ½mi body a postupnÄ› je sluÄuje.
- DÄ›livÃ©: zaÄÃ­nÃ¡ s jednÃ­m shlukem a postupnÄ› ho rozdÄ›luje.
VÃ½stupem je dendrogram.

2. **ShlukovÃ¡nÃ­ rozkladem (napÅ™. K-means):**
RozdÄ›lÃ­ data do pÅ™edem zadanÃ©ho poÄtu K shlukÅ¯ tak, aby body uvnitÅ™ shluku byly co nejpodobnÄ›jÅ¡Ã­. OpakovanÄ› aktualizuje stÅ™edy a pÅ™iÅ™azenÃ­ bodÅ¯ ke shlukÅ¯m.

## UvaÅ¾ujte dva body x= (1;2) a y = (4;6). SpoÄÃ­tejte jejich Eukleidovskou i Manhttanskou vzdÃ¡lenostâŒ

$\mathbf{x} = (1,\ 2)$
$\mathbf{y} = (4,\ 6)$

---

**EukleidovskÃ¡ vzdÃ¡lenost:**

$$
\sqrt{(4 - 1)^2 + (6 - 2)^2} = \sqrt{3^2 + 4^2} = \sqrt{9 + 16} = \sqrt{25} = 5
$$

**ManhattanskÃ¡ vzdÃ¡lenost:**

$$
|4 - 1| + |6 - 2| = 3 + 4 = 7
$$

---

**VÃ½sledek:**

* EukleidovskÃ¡ vzdÃ¡lenost = **5**
* ManhattanskÃ¡ vzdÃ¡lenost = **7**

## VÃ½poÄet entropie âŒ
MÃ¡me:

* $|S| = 100$
* $y = 0$: 30 zÃ¡znamÅ¯ â†’ $p_0 = 30/100 = 0{,}3$
* $y = 1$: 70 zÃ¡znamÅ¯ â†’ $p_1 = 70/100 = 0{,}7$

---

### **VÃ½poÄet entropie:**

$$
H(S) = -p_0 \cdot \log_2(p_0) - p_1 \cdot \log_2(p_1)
$$

$$
H(S) = -0{,}3 \cdot (-1{,}7) - 0{,}7 \cdot (-0{,}5) = 0{,}51 + 0{,}35 = \boxed{0{,}86}
$$

---

**OdpovÄ›Ä:** Entropie mnoÅ¾iny $S$ je **0,86**.


## StruÄnÄ› popiÅ¡te klasifikÃ¡tor k nejbliÅ¾Å¡Ã­ch sousedÅ¯ (k-nn). Jak probÃ­hÃ¡ uÄÃ­cÃ­ a klasifikaÄnÃ­ fÃ¡ze?âœ”ï¸ ProÄ je tento klasifikÃ¡tor nevhodnÃ½ pro velkÃ© trÃ©novacÃ­ mnoÅ¾iny?âŒ
**KlasifikÃ¡tor k nejbliÅ¾Å¡Ã­ch sousedÅ¯ (k-NN):**
PÅ™iÅ™azuje neznÃ¡mÃ©mu vzorku tÅ™Ã­du podle vÄ›tÅ¡iny z jeho **k nejbliÅ¾Å¡Ã­ch sousedÅ¯** v trÃ©novacÃ­ch datech (podle zvolenÃ© metriky, napÅ™. EukleidovskÃ© vzdÃ¡lenosti).

---

### **UÄÃ­cÃ­ fÃ¡ze:**

**NeprobÃ­hÃ¡ Å¾Ã¡dnÃ© uÄenÃ­.** TrÃ©novacÃ­ data se pouze uloÅ¾Ã­ (pamÄ›Å¥ovÃ½ pÅ™Ã­stup).

### **KlasifikaÄnÃ­ fÃ¡ze:**

1. SpoÄÃ­tajÃ­ se vzdÃ¡lenosti mezi novÃ½m vzorkem a vÅ¡emi trÃ©novacÃ­mi vzorky.
2. Vybere se k nejbliÅ¾Å¡Ã­ch sousedÅ¯.
3. PÅ™iÅ™adÃ­ se tÅ™Ã­da podle vÄ›tÅ¡iny z tÄ›chto sousedÅ¯.

---

### **NevÃ½hoda pro velkÃ© mnoÅ¾iny:**

VyÅ¾aduje **vÃ½poÄet vzdÃ¡lenostÃ­ ke vÅ¡em trÃ©novacÃ­m vzorkÅ¯m**, coÅ¾ je **ÄasovÄ› nÃ¡roÄnÃ©** (pomalÃ¡ klasifikace).  
TakÃ© **pamÄ›Å¥ovÄ› nÃ¡roÄnÃ©**, protoÅ¾e se uklÃ¡dajÃ­ vÅ¡echna trÃ©novacÃ­ data.

### JakÃ¡ souÄÃ¡st pÅ™Ã­pravy dat pÅ™ed shlukovÃ¡nÃ­m je obykle nezbytnÃ¡ a proÄ?
**Nezbytnou souÄÃ¡stÃ­ pÅ™Ã­pravy dat pÅ™ed shlukovÃ¡nÃ­m je normalizace (Å¡kÃ¡lovÃ¡nÃ­) promÄ›nnÃ½ch.**

---

### **DÅ¯vod:**

VÄ›tÅ¡ina shlukovacÃ­ch algoritmÅ¯ (napÅ™. K-means) pouÅ¾Ã­vÃ¡ **vzdÃ¡lenosti** mezi body.
**PromÄ›nnÃ© s vÄ›tÅ¡Ã­m rozsahem** by ovlivnily vÃ½poÄet vzdÃ¡lenostÃ­ **vÃ­ce** neÅ¾ promÄ›nnÃ© s menÅ¡Ã­m rozsahem.
NormalizacÃ­ (napÅ™. na rozsah âŸ¨0,1âŸ© nebo pomocÃ­ z-skÃ³re) se **vÅ¡em promÄ›nnÃ½m pÅ™idÄ›lÃ­ stejnÃ¡ vÃ¡ha**.

---

### **ShrnutÃ­:**

**Normalizace zajiÅ¡Å¥uje, Å¾e Å¾Ã¡dnÃ¡ promÄ›nnÃ¡ nedominuje vÃ½poÄtu vzdÃ¡lenostÃ­ â†’ sprÃ¡vnÃ© shluky.**


## **RegresnÃ­ koeficienty:**âœ”ï¸
Test 4

* **Intercept (Î²â‚€)** = âˆ’47.3060
* **Koeficient pro vÃ½Å¡ku (Î²â‚)** = 0.3385

---

### **FormÃ¡lnÃ­ zÃ¡pis (rovnice) modelu:**

$$
\text{tuk}_{\text{fat}} = -47.3060 + 0.3385 \cdot \text{tuk}_{\text{height}}
$$

---

Tedy: **Pro kaÅ¾dÃ© zvÃ½Å¡enÃ­ vÃ½Å¡ky o 1 jednotku vzroste procento tÄ›lesnÃ©ho tuku v prÅ¯mÄ›ru o 0.3385 %.**

## **StatistickÃ¡ vÃ½znamnost (p-hodnota) pro promÄ›nnou height:** âœ”ï¸
test 4

Z vÃ½stupu je:

$$
p = 0{,}0126
$$

---

### **HodnocenÃ­ vÃ½znamnosti pro Î± = 0,01:**

ProtoÅ¾e

$$
p = 0{,}0126 > 0{,}01,
$$

**nezamÃ­tÃ¡me nulovou hypotÃ©zu**, tedy **koeficient nenÃ­ statisticky vÃ½znamnÃ½** na hladinÄ› vÃ½znamnosti 0,01.

---

## **OdpovÄ›Ä do testu:**
test 4

* **DosaÅ¾enÃ¡ hladina vÃ½znamnosti je 0,0126.**
* **Koeficient nenÃ­ statisticky vÃ½znamnÃ½ na hladinÄ› 0,01, protoÅ¾e p > 0,01.**


Z tabulky mÃ¡me: âœ”ï¸

* **TP (true positive)** = 5
* **TN (true negative)** = 2
* **FP (false positive)** = 1
* **FN (false negative)** = 2
* **Celkem vzorkÅ¯** = 10

---

### **1. KlasifikaÄnÃ­ pÅ™esnost (accuracy):**

$$
\frac{TP + TN}{TP + TN + FP + FN} = \frac{5 + 2}{10} = \frac{7}{10}
$$

---

### **2. RelativnÃ­ chyba (error rate):**

$$
\frac{FP + FN}{TP + TN + FP + FN} = \frac{1 + 2}{10} = \frac{3}{10}
$$

---

### **3. Sensitivita (recall, true positive rate):**

$$
\frac{TP}{TP + FN} = \frac{5}{5 + 2} = \frac{5}{7}
$$

---

### **4. Specificita (true negative rate):**

$$
\frac{TN}{TN + FP} = \frac{2}{2 + 1} = \frac{2}{3}
$$

---

**ShrnutÃ­ odpovÄ›dÃ­:**

* **PÅ™esnost:** $\frac{7}{10}$
* **RelativnÃ­ chyba:** $\frac{3}{10}$
* **Sensitivita:** $\frac{5}{7}$
* **Specificita:** $\frac{2}{3}$


## **OtÃ¡zka:** âœ”ï¸
test 5

JakÃ¡ je pravdÄ›podobnost sluneÄnÃ©ho poÄasÃ­ (**outlook = sunny**) za pÅ™edpokladu, Å¾e hrÃ¡Äi **neÅ¡li hrÃ¡t tenis (play = no)**?

---

### **Postup (podmÃ­nÄ›nÃ¡ pravdÄ›podobnost):**

PoÄet pÅ™Ã­padÅ¯, kdy **play = no**:
â†’ z tabulky vidÃ­me: **5 pÅ™Ã­padÅ¯**

Z nich kolikrÃ¡t byl **outlook = sunny**?
â†’ **3 pÅ™Ã­pady** (Å™Ã¡dky: 1, 2, 7)

---

### **VÃ½poÄet:**

$$
P(\text{sunny} \mid \text{play=no}) = \frac{\text{poÄet pÅ™Ã­padÅ¯ (sunny âˆ§ no)}}{\text{poÄet vÅ¡ech pÅ™Ã­padÅ¯ (no)}} = \frac{3}{5}
$$

---

### **OdpovÄ›Ä:**

$$
\boxed{\frac{3}{5} \text{ nebo } 60\%}
$$

## UvaÅ¾ujete klasifikaÄnÃ­ algoritmus k-NN. PopiÅ¡te jak se urÄuje optimÃ¡lnÃ­ hodnota, mÃ¡te-li k dispozici soubor dat s pÅ™edem urÄenou klasifikacÃ­. Jak nejlÃ©pe urÄitÃ­ koneÄnou klasifikaÄnÃ­ pÅ™esnost modelu pro vybranÃ© kâœ”ï¸
OptimÃ¡lnÃ­ hodnota k se urÄuje pomocÃ­ kÅ™Ã­Å¾ovÃ© validace, kdy se porovnÃ¡ pÅ™esnost pro rÅ¯znÃ¡ k a zvolÃ­ se nejlepÅ¡Ã­. KoneÄnÃ¡ pÅ™esnost se urÄÃ­ opÄ›t kÅ™Ã­Å¾ovou validacÃ­ nebo pomocÃ­ testovacÃ­ mnoÅ¾iny.

## PopiÅ¡te k-nÃ¡sobnou kÅ™Ã­Å¾ovou validaci. Jak probÃ­hÃ¡ tvorba trÃ©novacÃ­ a testovacÃ­ mnoÅ¾iny pÅ™i pouÅ¾itÃ­ tÃ©to metody? Jak se spoÄÃ­tÃ¡ vÃ½slednÃ½ odhad chyby zvolenÃ©ho modeluâœ”ï¸
PÅ™i k-nÃ¡sobnÃ© kÅ™Ã­Å¾ovÃ© validaci se data rozdÄ›lÃ­ na k ÄÃ¡stÃ­. KaÅ¾dÃ¡ ÄÃ¡st je jednou testovacÃ­ a zbytek trÃ©novacÃ­. VÃ½slednÃ½ odhad chyby je prÅ¯mÄ›r chyb ze vÅ¡ech k iteracÃ­.

## PopiÅ¡te, co znÃ¡zorÅˆuje ROC kÅ™ivka. JakÃ© metriky tvoÅ™Ã­ jejÃ­ osy? Co znamenÃ¡ bod [0,1] a proÄ je povaÅ¾ovÃ¡n za ideÃ¡lnÃ­?âŒ
### **ROC kÅ™ivka (Receiver Operating Characteristic):**

ZnÃ¡zorÅˆuje **schopnost klasifikÃ¡toru rozliÅ¡ovat mezi tÅ™Ã­dami** pÅ™i rÅ¯znÃ½ch prahovÃ½ch hodnotÃ¡ch.

---

### **Osy:**

* **Y-osa:** **True Positive Rate (TPR)** = TP / (TP + FN), tj. **senzitivita**
* **X-osa:** **False Positive Rate (FPR)** = FP / (FP + TN)

---

### **Bod \[0, 1]:**

* **FPR = 0** (Å¾Ã¡dnÃ© faleÅ¡nÃ© poplachy)
* **TPR = 1** (vÅ¡echny pozitivnÃ­ sprÃ¡vnÄ› zachyceny)

---

### **ProÄ je ideÃ¡lnÃ­:**

* ZnamenÃ¡ **dokonalou klasifikaci** â€“ model **nemÃ¡ Å¾Ã¡dnÃ© chyby**.

---

**ShrnutÃ­ do testu:**
ROC kÅ™ivka zobrazuje zÃ¡vislost mezi **TPR** a **FPR** pÅ™i rÅ¯znÃ½ch prahovÃ½ch hodnotÃ¡ch. Bod \[0,1] znaÄÃ­ **ideÃ¡lnÃ­ klasifikÃ¡tor** â€“ 100% senzitivita a 0% faleÅ¡nÃ¡ pozitivita.


## Jakou tÅ™Ã­du problÃ©mu Å™eÅ¡Ã­ jednoduchÃ½ perceptronovÃ½ algoritmus? JakÃ¡ jsou jeho omezenÃ­? Jak je leze pÅ™ekonat?âœ”ï¸
JednoduchÃ½ perceptron Å™eÅ¡Ã­ lineÃ¡rnÄ› separovatelnÃ© problÃ©my. NedokÃ¡Å¾e klasifikovat nelineÃ¡rnÄ› oddÄ›lenÃ© tÅ™Ã­dy. Å˜eÅ¡enÃ­m je pouÅ¾itÃ­ vÃ­cevrstvÃ© sÃ­tÄ› s nelineÃ¡rnÃ­mi funkcemi.

JasnÄ›, vysvÄ›tlÃ­m ti to ÃºplnÄ› jednoduÅ¡e:

---

## Co vidÃ­Å¡ na obrÃ¡zku?âœ”ï¸

* DvÄ› tÅ™Ã­dy bodÅ¯:

  * **ModrÃ©** ve stÅ™edu.
  * **ÄŒervenÃ©** kolem do kruhu.
* Tyto tÅ™Ã­dy **nejde oddÄ›lit pÅ™Ã­mkou**, protoÅ¾e jsou â€do kruhuâ€œ.

---

### ğŸ”¹ **1. Pokud mÃ¡Å¡ SVM s *lineÃ¡rnÃ­m jÃ¡drem*** (= umÃ­ kreslit jen **pÅ™Ã­mku**):

SVM si neumÃ­ s tÃ­mto poradit.
Abychom mu **pomohli**, musÃ­me **pÅ™edÄ›lat data**.
NapÅ™Ã­klad spoÄÃ­tÃ¡me pro kaÅ¾dÃ½ bod jeho **vzdÃ¡lenost od stÅ™edu** (xÂ² + yÂ²).

* Pak dostaneme hodnoty:

  * ModrÃ© majÃ­ malou vzdÃ¡lenost.
  * ÄŒervenÃ© vÄ›tÅ¡Ã­.
* Tohle uÅ¾ **lze oddÄ›lit pÅ™Ã­mkou** â†’ teÄ SVM s lineÃ¡rnÃ­m jÃ¡drem funguje.

---

### ğŸ”¸ **2. KdyÅ¾ pouÅ¾ijeme *lepÅ¡Ã­ jÃ¡dro* (napÅ™. RBF):**

* **RBF jÃ¡dro** automaticky â€ohne prostorâ€œ, aby oddÄ›lilo i tyhle sloÅ¾itÃ© tvary.
* **NemusÃ­Å¡ nic poÄÃ­tat navÃ­c**, SVM to zvlÃ¡dne samo.

---

### ShrnutÃ­:

* **SVM s lineÃ¡rnÃ­m jÃ¡drem** â†’ musÃ­Å¡ **pÅ™edÄ›lat data** (napÅ™. poÄÃ­tat vzdÃ¡lenost).
* **SVM s RBF jÃ¡drem** â†’ **nemusÃ­Å¡ nic mÄ›nit**, rovnou funguje.


## **1. Kolik modelÅ¯ lze vytvoÅ™it?**

MÃ¡me:

* 5 moÅ¾nostÃ­ pro **C**: {0.001, 0.01, 0.1, 1, 10}
* 3 moÅ¾nosti pro **d**: {5, 10, 15}

PoÄet kombinacÃ­ =

$$
5 \cdot 3 = \boxed{15}
$$

---

### **2. Jak vybrat nejlepÅ¡Ã­ kombinaci C a d (aby model nepÅ™euÄil)?**

#### **Postup krok za krokem:**

1. **RozdÄ›l data na trÃ©novacÃ­ a validaÄnÃ­ mnoÅ¾inu**
   â€“ napÅ™. pomocÃ­ **kÅ™Ã­Å¾ovÃ© validace** (napÅ™. 5-fold cross-validation).

2. **Pro kaÅ¾dou kombinaci C a d (celkem 15 modelÅ¯):**

   * NatrÃ©nuj model na trÃ©novacÃ­ch datech.
   * VyhodnoÅ¥ pÅ™esnost (nebo jinou metriku) na validaÄnÃ­ch datech.
   * Zaznamenej prÅ¯mÄ›rnÃ½ vÃ½sledek.

3. **Vyber kombinaci C a d s nejlepÅ¡Ã­ prÅ¯mÄ›rnou pÅ™esnostÃ­**
   â€“ tedy model, kterÃ½ mÃ¡ **nejlepÅ¡Ã­ vÃ½kon na validaÄnÃ­ch datech** â†’ nejlÃ©pe se **obecnÄ› nauÄil**.

4. **Znovu natrÃ©nuj finÃ¡lnÃ­ model s touto kombinacÃ­** na celÃ© trÃ©novacÃ­ mnoÅ¾inÄ› (bez dÄ›lenÃ­).

---

### **ProÄ tento postup?**

* KÅ™Ã­Å¾ovÃ¡ validace pomÃ¡hÃ¡ **odhalit pÅ™euÄenÃ­ (overfitting)**.
* HledÃ¡me **parametry, kterÃ© generalizujÃ­** â€“ tedy fungujÃ­ dobÅ™e i na novÃ½ch datech.

---
**StruÄnÄ› do testu:**

PouÅ¾ijeme **kÅ™Ã­Å¾ovou validaci** pro kaÅ¾dou z 15 kombinacÃ­ (C Ã— d). Vybereme tu kombinaci, kterÃ¡ mÃ¡ **nejniÅ¾Å¡Ã­ validaÄnÃ­ chybu** (nebo nejvyÅ¡Å¡Ã­ pÅ™esnost). TÃ­m minimalizujeme riziko **pÅ™euÄenÃ­** a najdeme **nejvhodnÄ›jÅ¡Ã­ model**.



## RozhodovacÃ­ funkce:âœ”ï¸
test 8

Model je jednoduchÃ½ **perceptron**, kterÃ½ rozhoduje podle lineÃ¡rnÃ­ kombinace vstupÅ¯ a biasu.

Z obrÃ¡zku:

* VÃ¡ha k $x$: **0.34268**
* VÃ¡ha k $y$: **0.3417**
* Bias (vÃ¡ha ke konstantÄ› 1): **2.12288**

---

RozhodovacÃ­ funkce:

$$
f(x, y) = 0.34268 \cdot x + 0.3417 \cdot y + 2.12288
$$

RozhodnutÃ­:

* Pokud $f(x, y) > 0$ â†’ tÅ™Ã­da **1**
* Pokud $f(x, y) \leq 0$ â†’ tÅ™Ã­da **-1**

---

###  **2. GeometrickÃ¡ interpretace rozhodovacÃ­ hranice:**

RozhodovacÃ­ hranice je **pÅ™Ã­mka** v rovinÄ› $(x, y)$, kterÃ¡ rozdÄ›luje prostor na dvÄ› ÄÃ¡sti:

* Na jednÃ© stranÄ› funkce $f(x, y) > 0$ â†’ tÅ™Ã­da 1
* Na druhÃ© stranÄ› $f(x, y) < 0$ â†’ tÅ™Ã­da -1

Jde tedy o **lineÃ¡rnÃ­ hranici (pÅ™Ã­mku)**, kterÃ¡ je definovÃ¡na rovnicÃ­:

$$
-0.34268 \cdot x + -0.3417 \cdot y + 2.12288 = 0
$$

---

###  **3. Do kterÃ© tÅ™Ã­dy model zaÅ™adÃ­ vzorek (x=1, y=2)?**

DosadÃ­me do rozhodovacÃ­ funkce:

$$
f(1, 2) = -0.34268 \cdot 1 + -0.3417 \cdot 2 + 2.12288 = \\
= 0.34268 + 0.6834 + 2.12288 = 3.14896 > 0
$$

â†’ VÃ½sledek je **kladnÃ½**, takÅ¾e vÃ½stupnÃ­ tÅ™Ã­da je:

$$
\boxed{1}
$$

---

 **ShrnutÃ­ odpovÄ›dÃ­ do testu:**

1. $f(x, y) = -0.34268 \cdot x + -0.3417 \cdot y + 2.12288$
2. RozhodovacÃ­ hranice je **pÅ™Ã­mka** rozdÄ›lujÃ­cÃ­ prostor na dvÄ› tÅ™Ã­dy.
3. Vzorek $(1, 2)$ model zaÅ™adÃ­ do tÅ™Ã­dy **1**.


## JakÃ½m zpÅ¯sobem se mÄ›nÃ­ vÃ¡hy v kaÅ¾dÃ© iteraci algoritmu AdaBoost?âœ”ï¸
### ğŸ” **VysvÄ›tlenÃ­ (struÄnÄ› pro test):**

V kaÅ¾dÃ© iteraci algoritmu **AdaBoost**:

* **VÃ¡hy Å¡patnÄ› klasifikovanÃ½ch vzorkÅ¯ se zvyÅ¡ujÃ­** â†’ aby se na nÄ› pÅ™Ã­Å¡tÃ­ klasifikÃ¡tor vÃ­ce soustÅ™edil.
* **VÃ¡hy sprÃ¡vnÄ› klasifikovanÃ½ch vzorkÅ¯ se sniÅ¾ujÃ­** â†’ protoÅ¾e uÅ¾ jsou â€snadnÃ©â€œ.

---

### ğŸ” TÃ­mto zpÅ¯sobem:

* DalÅ¡Ã­ slabÃ½ klasifikÃ¡tor se **vÃ­ce zamÄ›Å™Ã­ na obtÃ­Å¾nÃ© pÅ™Ã­klady**.
* PostupnÄ› se kombinuje vÃ­ce slabÃ½ch klasifikÃ¡torÅ¯ do jednoho silnÃ©ho modelu.


Zde jsou **struÄnÃ© a pÅ™esnÃ© odpovÄ›di** na otÃ¡zky o **PCA (AnalÃ½ze hlavnÃ­ch komponent):**

---

## **1. V jakÃ©m vztahu jsou komponenty (PCA) k pÅ¯vodnÃ­m souÅ™adnicÃ­m dat?**

â†’ **Jsou ortogonÃ¡lnÃ­ (navzÃ¡jem kolmÃ©)** a tvoÅ™Ã­ novou bÃ¡zi.
KaÅ¾dÃ¡ komponenta je lineÃ¡rnÃ­ kombinacÃ­ pÅ¯vodnÃ­ch promÄ›nnÃ½ch.

---

### **2. Podle jakÃ©ho kritÃ©ria se hledÃ¡ novÃ¡ bÃ¡ze dat v PCA?**

â†’ **Maximalizace rozptylu (variance)** â€“ vybÃ­rÃ¡ se takovÃ½ smÄ›r, ve kterÃ©m majÃ­ data nejvÄ›tÅ¡Ã­ rozptyl.
KaÅ¾dÃ¡ dalÅ¡Ã­ komponenta maximalizuje rozptyl za podmÃ­nky ortogonality k pÅ™edchozÃ­m.

---

### **3. Jak se problÃ©m PCA Å™eÅ¡Ã­ matematicky (zÃ¡kladnÃ­ myÅ¡lenka)?**

â†’ **VÃ½poÄtem vlastnÃ­ch vektorÅ¯ a vlastnÃ­ch ÄÃ­sel kovarianÄnÃ­ matice dat.**

* VlastnÃ­ vektory = smÄ›ry hlavnÃ­ch komponent
* VlastnÃ­ ÄÃ­sla = velikosti rozptylu v tÄ›chto smÄ›rech


## **1 JakÃ© kritÃ©rium optimalizuje LDA a jakÃ© dvÄ› vlastnosti popisuje?**

**KritÃ©rium LDA:**
Maximalizuje pomÄ›r **mezitÅ™Ã­dnÃ­ rozptyl / vnitrotÅ™Ã­dnÃ­ rozptyl**.

**Vlastnosti:**

* **MezitÅ™Ã­dnÃ­ rozptyl** mÃ¡ bÃ½t **co nejvÄ›tÅ¡Ã­** â†’ tÅ™Ã­dy jsou co nejdÃ¡l od sebe.
* **VnitrotÅ™Ã­dnÃ­ rozptyl** mÃ¡ bÃ½t **co nejmenÅ¡Ã­** â†’ vzorky uvnitÅ™ jednÃ© tÅ™Ã­dy jsou si podobnÃ©.

---

### **2 K Äemu se vyuÅ¾Ã­vÃ¡ LDA a kdy je lepÅ¡Ã­ neÅ¾ PCA?**

**LDA se pouÅ¾Ã­vÃ¡ pro:**
â†’ **Klasifikaci a redukci dimenze s ohledem na tÅ™Ã­dy (labely)**.

**LDA je vÃ½hodnÄ›jÅ¡Ã­ neÅ¾ PCA, kdyÅ¾:**
â†’ **MÃ¡me znÃ¡mÃ© tÅ™Ã­dy** a chceme **najÃ­t prostor, kterÃ½ je co nejlepÅ¡Ã­ pro jejich oddÄ›lenÃ­**.

**PÅ™Ã­klady Ãºloh:** RozpoznÃ¡vÃ¡nÃ­ obliÄejÅ¯, diagnostika podle oznaÄenÃ½ch dat (napÅ™. nemoc vs. zdravÃ½).

---

ShrnutÃ­ do testu:

1. LDA maximalizuje pomÄ›r mezitÅ™Ã­dnÃ­ho a vnitrotÅ™Ã­dnÃ­ho rozptylu.
2. VyuÅ¾Ã­vÃ¡ se pro klasifikaci, je vÃ½hodnÄ›jÅ¡Ã­ neÅ¾ PCA, pokud mÃ¡me znÃ¡mÃ© tÅ™Ã­dy.

## ğŸ” Ãšloha:

MÃ¡me rozhodovacÃ­ strom (Decision Stump), kterÃ½ dÄ›lÃ­ pouze podle jedinÃ© podmÃ­nky:

$$
x_1 < 4.7606 \Rightarrow \text{tÅ™Ã­da } 1  
$$

$$
x_1 \geq 4.7606 \Rightarrow \text{tÅ™Ã­da } 0
$$

---

### ğŸ“Œ NovÃ½ vstupnÃ­ vektor:

$$
x = (x_1, x_2) = (2,\ 8)
$$

â†’ $x_1 = 2 < 4.7606$

---

### âœ… RozhodnutÃ­:

Model pÅ¯jde po vÄ›tvi **YES â†’ xâ‚ < 4.7606**
â†’ tÅ™Ã­da podle uzlu Ä. 3:

$$
y = \boxed{1}
$$

---

### âœï¸ OdpovÄ›Ä do testu:

**Pro vektor $x = (2,\ 8)$ se model rozhodne pro tÅ™Ã­du 1.**


PodÃ­vÃ¡me se na odpovÄ›di podle obrÃ¡zku a tabulky transakcÃ­:

---

### ğŸ”¹ **1. Podpora mnoÅ¾iny {mlÃ©ko, chlÃ©b, pleny}**

Projdeme transakce a spoÄÃ­tÃ¡me, kolik z nich obsahuje **vÅ¡echny tÅ™i poloÅ¾ky: mlÃ©ko, chlÃ©b a pleny**.

* Transakce 1: chlÃ©b, mlÃ©ko â†’ âŒ
* Transakce 2: chlÃ©b, pleny, pivo, vejce â†’ âŒ
* Transakce 3: mlÃ©ko, pleny, pivo, kola â†’ âŒ
* Transakce 4: chlÃ©b, mlÃ©ko, pleny, pivo â†’ âœ…
* Transakce 5: chlÃ©b, mlÃ©ko, pleny, kola â†’ âœ…

âœ… â†’ ve **2 transakcÃ­ch z 5**

$$
\text{Support} = \frac{2}{5}
$$

---

### ğŸ”¸ **2. AsociaÄnÃ­ pravidlo: {mlÃ©ko, pleny} â‡’ pivo**

#### **Support:**

HledÃ¡me, ve **kolika transakcÃ­ch jsou vÅ¡echny tÅ™i poloÅ¾ky: mlÃ©ko, pleny, pivo**:

* Transakce 3: mlÃ©ko, pleny, pivo â†’ âœ…
* Transakce 4: mlÃ©ko, pleny, pivo â†’ âœ…
* Transakce 5: mlÃ©ko, pleny, kola â†’ âŒ (chybÃ­ pivo)

â†’ **2 transakce z 5**

$$
\text{Support} = \frac{2}{5}
$$

---

#### **Confidence (spolehlivost):**

Ze vÅ¡ech transakcÃ­, kterÃ© obsahujÃ­ **mlÃ©ko a pleny**, v kolika je i **pivo**?

* Transakce 3: mlÃ©ko, pleny â†’ âœ… (obsahuje pivo)
* Transakce 4: mlÃ©ko, pleny â†’ âœ… (obsahuje pivo)
* Transakce 5: mlÃ©ko, pleny â†’ âœ… (NEobsahuje pivo)

â†’ 3 transakce s {mlÃ©ko, pleny}, z toho 2 obsahujÃ­ pivo

$$
\text{Confidence} = \frac{2}{3} \approx \boxed{66{,}7\%}
$$

---

### âœ… **ShrnutÃ­ vÃ½sledkÅ¯:**

* **Support:** $\frac{2}{5}$
* **Confidence:** $\frac{2}{3} = 66{,}7\%$












